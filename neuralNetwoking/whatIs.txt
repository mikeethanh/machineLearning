Bài trước học về thuật toán logistic regression với giá trị đầu ra là giá trị nhị phân. Tuy nhiên,
logistic regression là một mô hình neural network đơn giản, bài này sẽ học mô hình neural network
đầy đủ

Con chó có thể phân biệt được người thân trong gia đình và người lạ hay đứa trẻ có thể phân biệt được
các con vật. Những việc tưởng chừng như rất đơn giản nhưng lại cực kì khó để thực hiện bằng máy
tính. Vậy sự khác biệt nằm ở đâu? Câu trả lời nằm ở cấu trúc bộ não với lượng lớn các nơ-ron thần
kinh liên kết với nhau. Liệu máy tính có thể mô phỏng lại cấu trúc bộ não để giải các bài toán trên ???
Neural là tính từ của neuron (nơ-ron), network chỉ cấu trúc, cách các nơ-ron đó liên kết với
nhau, nên neural network (NN) là một hệ thống tính toán lấy cảm hứng từ sự hoạt động của các
nơ-ron trong hệ thần kinh.

5.1.1 Hoạt động của các nơ-ron
Nơ-ron là đơn vị cơ bản cấu tạo hệ thống thần kinh và là thành phần quan trọng nhất của não. Đầu
chúng ta gồm khoảng 10 triệu nơ-ron và mỗi nơ-ron lại liên kết với tầm 10.000 nơ-ron khác.
Ở mỗi nơ-ron có phần thân (soma) chứa nhân, các tín hiệu đầu vào qua sợi nhánh (dendrites)
và các tín hiệu đầu ra qua sợi trục (axon) kết nối với các nơ-ron khác. Hiểu đơn giản mỗi nơ-ron
nhận dữ liệu đầu vào qua sợi nhánh và truyền dữ liệu đầu ra qua sợi trục, đến các sợi nhánh của các
nơ-ron khác.

Mỗi nơ-ron nhận xung điện từ các nơ-ron khác qua sợi nhánh. Nếu các xung điện này đủ lớn
để kích hoạt nơ-ron, thì tín hiệu này đi qua sợi trục đến các sợi nhánh của các nơ-ron khác. => Ở
mỗi nơ-ron cần quyết định có kích hoạt nơ-ron đấy hay không. Tương tự các hoạt động của hàm
sigmoid bài trước.

Tuy nhiên NN chỉ là lấy cảm hứng từ não bộ và cách nó hoạt động, chứ không phải bắt chước toàn
bộ các chức năng của nó. Việc chính của chúng ta là dùng mô hình đấy đi giải quyết các bài toán
chúng ta cần

Hàm sigmoid ở đây được gọi là activation function

5.2.2 Mô hình tổng quát
Layer đầu tiên là input layer, các layer ở giữa được gọi là hidden layer, layer cuối cùng được gọi là
output layer. Các hình tròn được gọi là node.
Mỗi mô hình luôn có 1 input layer, 1 output layer, có thể có hoặc không các hidden layer. Tổng số
layer trong mô hình được quy ước là số layer - 1 (không tính input layer).
Ví dụ như ở hình trên có 1 input layer, 2 hidden layer và 1 output layer. Số lượng layer của
mô hình là 3 layer.
Mỗi node trong hidden layer và output layer :
• Liên kết với tất cả các node ở layer trước đó với các hệ số w riêng.
• Mỗi node có 1 hệ số bias b riêng.
• Diễn ra 2 bước: tính tổng linear và áp dụng activation function.

5.2.2 Mô hình tổng quát
Layer đầu tiên là input layer, các layer ở giữa được gọi là hidden layer, layer cuối cùng được gọi là
output layer. Các hình tròn được gọi là node.
Mỗi mô hình luôn có 1 input layer, 1 output layer, có thể có hoặc không các hidden layer. Tổng số
layer trong mô hình được quy ước là số layer - 1 (không tính input layer).
Ví dụ như ở hình trên có 1 input layer, 2 hidden layer và 1 output layer. Số lượng layer của
mô hình là 3 layer.
Mỗi node trong hidden layer và output layer :
• Liên kết với tất cả các node ở layer trước đó với các hệ số w riêng.
• Mỗi node có 1 hệ số bias b riêng.
• Diễn ra 2 bước: tính tổng linear và áp dụng activation function.

5.3 Feedforward

5.3.1 Biểu diễn dưới dạng ma trận
Tuy nhiên khi làm việc với dữ liệu ta cần tính dự đoán cho nhiều dữ liệu một lúc, nên gọi X là ma
trận n*d, trong đó n là số dữ liệu và d là số trường trong mỗi dữ liệu, trong đó x
[i]
j
là giá trị trường
dữ liệu thứ j của dữ liệu thứ i.

Giờ từ input X ta có thể tính được giá trị dự đoán Yˆ, tuy nhiên việc chính cần làm là đi tìm
hệ số W và b. Có thể nghĩ ngay tới thuật toán gradient descent và việc quan trọng nhất trong thuật
toán gradient descent là đi tìm đạo hàm của các hệ số đối với loss function. Và việc tính đạo hàm
của các hệ số trong neural network được thực hiện bởi thuật toán backpropagation, sẽ được giới
thiệu ở bài sau. Và vì bài này có quá nhiều công thức sợ mọi người rối nên code sẽ được để ở bài
sau

5.4 Logistic regression với toán tử XOR